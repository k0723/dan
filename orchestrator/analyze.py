from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
import hdbscan
import numpy as np
import re
import time
from collections import defaultdict
from keybert import KeyBERT

# ëª¨ë¸ ì´ˆê¸°í™”
model = SentenceTransformer('all-MiniLM-L6-v2')
kw_model = KeyBERT(model='all-MiniLM-L6-v2')

# ì¡°ì‚¬ ì œê±°ìš© ë¦¬ìŠ¤íŠ¸
POSTPOSITIONS = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì—ê²Œ", "ë¡œ", "ìœ¼ë¡œ", "ê³¼", "ì™€", "ë„", "ë§Œ", "ê¹Œì§€", "ë¶€í„°"]

# ì¡°ì‚¬ ì œê±° í•¨ìˆ˜
def clean_postpositions(word: str) -> str:
    return re.sub(f"({'|'.join(POSTPOSITIONS)})$", '', word)

# LLM ë°±ì—… í‚¤ì›Œë“œ í•¨ìˆ˜
def fallback_keyword_with_llm(text: str) -> str:
    prompt = f"""
    ë¬¸ì¥ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ í•˜ë‚˜ë§Œ ë½‘ì•„ì¤˜. ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ëŠ” ì œê±°í•˜ê³ , ë‹¨ì–´ í•˜ë‚˜ë¡œë§Œ ë°˜í™˜í•´:
    ë¬¸ì¥: "{text}"
    í‚¤ì›Œë“œ:
    """
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("âŒ LLM ì‹¤íŒ¨:", e)
        return ""

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜

def extract_keywords(texts: list[str], timeout: float = 3.0) -> list[str]:
    raw_keywords = []

    for text in texts:
        try:
            print("ğŸ” ì…ë ¥ ë¬¸ì¥:", text)
            start_time = time.time()

            keywords = kw_model.extract_keywords(text, top_n=1)
            print("ğŸ§  KeyBERT ê²°ê³¼:", keywords)
            if not keywords or not keywords[0]:
                raise ValueError("kw_model ì‹¤íŒ¨")

            raw_keyword = keywords[0][0]
            cleaned = clean_postpositions(raw_keyword)
            print("ğŸ§¼ ì¡°ì‚¬ ì œê±° í›„:", cleaned)

            if len(cleaned) < 2 or not cleaned.isalnum():
                raise ValueError("í‚¤ì›Œë“œ ë„ˆë¬´ ì§§ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ")

            if time.time() - start_time > timeout:
                raise TimeoutError("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼")

            raw_keywords.append(cleaned)

        except Exception as e:
            print("âš ï¸ ì˜ˆì™¸ ë°œìƒ:", e)
            keyword = fallback_keyword_with_llm(text)
            cleaned = clean_postpositions(keyword)
            print("ğŸ¤– LLM ëŒ€ì²´ í‚¤ì›Œë“œ:", cleaned)
            if cleaned:
                raw_keywords.append(cleaned)

    # âœ… ì¡°ì‚¬ ì œê±° í›„ ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±°
    unique_keywords = []
    seen = set()
    for kw in raw_keywords:
        base = clean_postpositions(kw)
        if base and base not in seen:
            seen.add(base)
            unique_keywords.append(base)

    return unique_keywords


# ì „ì²´ ë¶„ì„ í•¨ìˆ˜ (í´ëŸ¬ìŠ¤í„°ë§ + í‚¤ì›Œë“œ)
def analyze_texts(texts: list[str]) -> dict:
    # 1. ì„ë² ë”©
    embeddings = model.encode(texts, show_progress_bar=True)
    embeddings = normalize(embeddings)

    # 2. HDBSCAN êµ°ì§‘í™”
    best_score = -1
    best_labels = None
    best_size = None
    for size in range(2, min(10, len(texts))):
        clusterer = hdbscan.HDBSCAN(min_cluster_size=size)
        labels = clusterer.fit_predict(embeddings)

        if len(set(labels)) <= 1 or all(l == -1 for l in labels):
            continue

        try:
            score = silhouette_score(embeddings, labels)
            if score > best_score:
                best_score = score
                best_labels = labels
                best_size = size
        except:
            continue

    # 3. êµ°ì§‘ ì‹¤íŒ¨ ì‹œ fallback
    if best_labels is None:
        return {
            "clusters": {
                f"Text {i}": {
                    "summary": text[:100] + "...",
                    "cluster": -1
                } for i, text in enumerate(texts)
            },
            "keywords": extract_keywords(texts)
        }

    # 4. êµ°ì§‘ ê²°ê³¼ êµ¬ì„±
    clusters = defaultdict(list)
    valid_texts = []
    for text, label in zip(texts, best_labels):
        clusters[label].append(text)
        if label != -1:
            valid_texts.append(text)

    cluster_info = {}
    for label, cluster_texts in clusters.items():
        summary = cluster_texts[0][:100] + "..."
        cluster_info[f"Cluster {label}"] = {
            "count": len(cluster_texts),
            "summary": summary,
            "min_cluster_size": best_size
        }

    # 5. í‚¤ì›Œë“œ ì¶”ì¶œ (í´ëŸ¬ìŠ¤í„°ë§ ëœ í…ìŠ¤íŠ¸ë§Œ)
    keywords = extract_keywords(valid_texts)

    return {
        "clusters": cluster_info,
        "keywords": keywords
    }
